{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning sentence representations from natural language inference data\n",
    "### Advanced Topics in Computational Semantics - Practical I - Ard Snijders - 12854913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ardsnijders/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_printoptions(threshold=100)\n",
    "import pickle\n",
    "\n",
    "import nltk.tokenize \n",
    "nltk.download('punkt')\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import senteval\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook overview\n",
    "\n",
    "This interactive notebook is comprised of four major sections: \n",
    "\n",
    "1. Overview of models\n",
    "2. Loading pre-trained models for inference\n",
    "3. Results on SNLI and SentEval\n",
    "4. Error Analysis (including mini perturbation study!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can load in pretrained models, we must first define what they should look like (to my knowledge, that is - I'm not sure if it's possible to load model checkpoints without first creating instances of them, and to create instances, the classes need to be defined somewhere - and I can't import them from another module in a jupyter notebook.\n",
    "\n",
    "For classification we use a simple Multi-Layer Perceptron - the size of its input dimension is dependent on the encoder model coupled to it. The MLP takes as its input the transformed combination of sentence vectors u and v. The MLP has a single hidden dimension of size 512, after which the input is projected onto a final 3-way layer, one for each of the labels. I chose to use a leakyReLu activation function between the two linear projections, as I found it slightly improved my accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for Multi-Layer Perceptron Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the network\n",
    "        :param input_dim: dimension of first layer\n",
    "        :param hidden_dim: dimension of hidden layer\n",
    "        \"\"\"\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Method to do the forward pass\n",
    "        :param input: batch of concatenations of [u,v,u-v,u*v]\n",
    "        :return: [batch_size, 3] tensor with predictions\n",
    "        \"\"\"\n",
    "\n",
    "        output = self.network(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Embedding Sentence Encoder\n",
    "The simplest sentence representation can be achieved by summing the embeddings of each word in the sentence, and then dividing this by the length of the sequence to get the average embedding for the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple network that takes average of all word embeddings in the sentence\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "\n",
    "        super(AverageEmbeddings, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_dim, mode=\"mean\")\n",
    "        self.embedding.weight.requires_grad=False\n",
    "\n",
    "    def forward(self, sentences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        the forward pass of the model\n",
    "        :param sentences: sequence representing hypothesis/premise sentence\n",
    "        :param offsets: length of offset\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sequence_lengths = None\n",
    "        embedded = self.embedding(sentences, sequence_lengths)\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Sentence Encoder\n",
    "\n",
    "We can incorporate more linguistic information by using a sequential model such as the LSTM. The LSTM takes as its input a sequence of embeddings and processes it sequentially, meaning that words are fed through the network one at a time; each pass for a single word is called a timestep, and for a sequence of K words the LSTM will go through a series of K timesteps to process that sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for all LSTM encoder models\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim, num_layers, bidirectional, maxpool):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM network\n",
    "        :param vocab_size: size of the vocabulary for initializing nn.Embedding\n",
    "        :param hidden_dim: dimensionality of hidden layer in LSTM\n",
    "        :param emb_dim: dimensionality of word embeddings\n",
    "        :param num_layers: number of \"stacked\" LSTM layers (always 1)\n",
    "        :param bidirectional: flag to turn on/off bidirectional encoding\n",
    "        \"\"\"\n",
    "        super(LSTM_Encoder, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.maxpool = maxpool\n",
    "\n",
    "        # Initialize embedding layers. Turn off gradient tracking as we don't want to update the embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.embedding.weight.requires_grad=False\n",
    "\n",
    "        # Layers to initialize hidden state & cell state(?)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=0.5)\n",
    " \n",
    "\n",
    "    def forward(self, sentences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the network\n",
    "        :param sentences: batch of sequences representing hypotheses or premises sentences\n",
    "        :param offsets: tensor with number of padding tokens per sentence\n",
    "        :return:\n",
    "        - if unidirectional: ([batch_size, hidden_dim]) shaped tensor with last hidden state of network\n",
    "        - if bidirectional: ([batch_size, 2*hidden_dim]) shaped tensor with concatenated forward/backward hidden states\n",
    "        - if bidirectional and max_pooling: ([batch_size, 2*hidden_dim]) tensor with max-pool on features for all t\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(sentences)\n",
    "\n",
    "        #this function packs sequences in batch into tuple of two lists with\n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, batch_first=True, \n",
    "                                                                  lengths=sequence_lengths, \n",
    "                                                                  enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.encoder(packed_embedded)\n",
    "\n",
    "        # Pad the packed output which contains the hidden states\n",
    "        # Function returns a tuple with padded outputs and tensor with lengths of each sequence in the batch\n",
    "        output_padded, seq_lengths = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, \n",
    "                                                                            padding_value=-9999)\n",
    "        # Bidirectional LSTM w/ Max-pooling on word level features\n",
    "        if self.bidirectional and self.maxpool:\n",
    "            batch = output_padded\n",
    "            batch = [(torch.max(sent, dim=0, keepdim=True)[0]).squeeze() for sent in batch]\n",
    "            \n",
    "            # Combine list of tensors back into tensor of tensors of shape ([batch_size, hidden_dim*2])\n",
    "            batch = torch.stack(batch, dim=0)\n",
    "            result = batch\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        elif self.bidirectional and not self.maxpool:\n",
    "            forward_h_s = hidden[0]\n",
    "            backward_h_s = hidden[1]\n",
    "            result = torch.cat((forward_h_s, backward_h_s), 1)\n",
    "        \n",
    "        # Uni-directional LSTM\n",
    "        else:\n",
    "            result = torch.squeeze(hidden)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training details and hyperparameters\n",
    "\n",
    "I largely chose to stick with the hyperparameters as provided by Conneau et al (2017). That is;\n",
    "\n",
    "- MLP with a single hidden layer of dim 512\n",
    "- LSTM with 2048 hidden units\n",
    "- Initial learning rate of 0.1. Learning rate is divided by 5 when validation accuracy stops increasing. \n",
    "\n",
    "I also diverge from them in some respects:\n",
    "\n",
    "- LeakyReLU in MLP seemed to give slightly better performance on the validation set\n",
    "- p = 0.5 dropout for the nn.encoder in the LSTM network gave similar improvements\n",
    "\n",
    "I trained all models for 25 epochs on Lisa; however, I found that this was largely overkill; most models seemed to have converged after about half of that already. I settled on checkpoints when I no longer saw significant increases in the validation accuracy:\n",
    "\n",
    "- average embeddings = 15 epochs\n",
    "- uni_LSTM = 12 epochs\n",
    "- bi_LSTM = 13 epochs\n",
    "- bi_LSTM w/ MP = 12 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading pre-trained models for inference \n",
    "\n",
    "### Loading Glove Vectors used during training\n",
    "\n",
    "I pickled the glove vectors that were used to initialize embedding weights during training - we need to load them first before we can initialize and load the desired models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors():   \n",
    "    with open('glovevecs.pickle', 'rb') as handle:\n",
    "        GloveVecs = pickle.load(handle)\n",
    "    \n",
    "    return GloveVecs\n",
    "GloveVecs = load_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading sentence encoder & MLP classifier\n",
    "In order to load the sentence encoder and MLP classifier, we must first create instances of each, with the appropriate parameters (e.g. the dimensionality of the input of the MLP should match the output of the encoder). We then load the model checkpoints. This function returns the pretrained encoder model and corresponding classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, emb_dim, hidden_dim, epochs, bidirectional, maxpool):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    model: str parameter that specifies whether to load average embedding or LSTM\n",
    "    emb_dim: int parameter that specifies dimensionality of word embeddings\n",
    "    hidden_dim: int parameter that specifies number of hidden units in MLP/LSTM\n",
    "    epochs: int parameter to specify for which training epoch we want to load the models.\n",
    "            this is 25 for all models, except for the bi-LSTM, which seemed to converge around 19 epochs already.\n",
    "    bidirectional: bool parameter to specify whether we want to load unidirectional or bidirectional LSTM\n",
    "    maxpool: bool parameter to specify whether we want bi-LSTM with or without maxpooling\n",
    "    \n",
    "    returns: encoder and classifier instances loaded with weights from the given training epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create instances of desired encoder and corresponding classifier, then load checkpoint files.\n",
    "    if model_type == 'AVERAGE':                                                                                                                                                  \n",
    "                                                                                                                                                                               \n",
    "        # Instantiate sentence encoder, populate with Glove embeddings.                                                                                                            \n",
    "        model = AverageEmbeddings(vocab_size=vocab_size, emb_dim=emb_dim)                                                                                                   \n",
    "        model.embedding.weight.data.copy_(GloveVecs)                                                                                                                               \n",
    "\n",
    "        # Instantiate classifier                                                                                                                                                   \n",
    "        input_dim = int(emb_dim * 4)                                                                                                                                        \n",
    "        classifier = MLPClassifier(input_dim=input_dim, hidden_dim=512)\n",
    "        \n",
    "        classifier_path = \"./models/{}/classifier_checkpoint_epoch_{}.pt\".format(model_type, epochs)\n",
    "        classifier.load_state_dict(torch.load(classifier_path, map_location=torch.device('cpu')))                                                                                                                                       \n",
    "                                                                                                                                                                               \n",
    "    elif model_type == 'LSTM':                                                                                                                                                   \n",
    "                                                                                                                                                                               \n",
    "        # Instantiate sentence encoder, populate with Glove embeddings.                                                                                                            \n",
    "        model = LSTM_Encoder(vocab_size=vocab_size, hidden_dim=hidden_dim, emb_dim=emb_dim, \n",
    "                             num_layers=1, bidirectional=bidirectional, maxpool=maxpool)\n",
    "        model.embedding.weight.data.copy_(GloveVecs)\n",
    "        \n",
    "        # Load checkpoint file for LSTM encode\n",
    "        model_path = \"./models/{}_bidirectional:_{}_maxpool:_{}/model_checkpoint_epoch_{}.pt\".format(\n",
    "                model_type, bidirectional, maxpool, epochs)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "        # Instantiate classifier                                                                                                                                                   \n",
    "        if not bidirectional:                                                                                                                                               \n",
    "            input_dim = int(4 * hidden_dim)                                                                                                                                 \n",
    "        else:                                                                                                                                                                      \n",
    "            input_dim = int(4 * hidden_dim * 2)\n",
    "        classifier = MLPClassifier(input_dim=input_dim, hidden_dim=512)\n",
    "        \n",
    "        # Load checkpoint file for MLP\n",
    "        classifier_path = \"./models/{}_bidirectional:_{}_maxpool:_{}/classifier_checkpoint_epoch_{}.pt\".format(\n",
    "                model_type, bidirectional, maxpool, epochs)\n",
    "        classifier.load_state_dict(torch.load(classifier_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    return model, classifier                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the vocabulary\n",
    "\n",
    "We need to load the vocabulary used during training - we need to know what ids to assign for each word in our new sentences. We also need to know the vocab size to initialize the embedding layers of our encoder instances with the right dimensionality. I extracted the defaultdict attribute from the torchtext.vocab.Vocab object with _object_.vocab.stoi - torchtext.vocab.Vocab itself is an attribute from the torchtext.data.field.field object, which was used to build the vocab during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():   \n",
    "    with open('vocabulary.pickle', 'rb') as handle:\n",
    "        vocab = pickle.load(handle)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36544 unique words.\n"
     ]
    }
   ],
   "source": [
    "vocab = load_vocab()\n",
    "print('Vocabulary size:', len(vocab.keys()), 'unique words.')\n",
    "\n",
    "vocab_size = len(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting sentence pairs into sequence tensors\n",
    "The *sent2seq* function serves to convert sentences into sequences of word_ids that correspond to the ids the model was trained on, employing the vocab object that was used during training, such that sequences can be handled appropriately by the embedding layers of the encoders.\n",
    "\n",
    "Because my original models expect batches of datapoints, and we're only feeding in pairs of sentences created \"on the spot\", I use *artifical_batch* to 'upsample' the example sentences into a single batch of 64 identical sequence tensors per sentence, so that I don't have to alter my model architecture too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artificial_batch(sequence, unsq):\n",
    "    seq_tensor = torch.tensor(sequence, dtype=torch.long)      \n",
    "    seq_tensor = seq_tensor.unsqueeze(0)\n",
    "    seq_tensor = seq_tensor.repeat(64,1)\n",
    "    if unsq == False:\n",
    "        seq_tensor = seq_tensor.squeeze(1)\n",
    "    return seq_tensor\n",
    "\n",
    "def sent2seq(sentences, vocab):\n",
    "    # Input = pair of strings where strings are premise and hypothesis sentences, respectively\n",
    "    # Empty list to store sequences of ids\n",
    "    prep_sentences = []\n",
    "    sequence_lengths = []  \n",
    "    for sentence in sentences:     \n",
    "        # Preprocess sentence: tokenization and lowercasing. \n",
    "        sentence = nltk.tokenize.word_tokenize(sentence.lower())        \n",
    "        # Create list for sequence for current sentence\n",
    "        prep_sentence = []     \n",
    "        # Convert sequence to sequence by look-up in vocab\n",
    "        # If word not in vocab, assign value associated with '<unk>'   \n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                prep_sentence.append(vocab[word])\n",
    "            else:\n",
    "                prep_sentence.append(vocab['<unk>'])            \n",
    "        # Convert list into [64, <len_list>] tensor for both sentence and seq lengths\n",
    "        seq_batch = artificial_batch(prep_sentence, unsq=True)\n",
    "        len_batch = artificial_batch(len(prep_sentence), unsq=False)       \n",
    "        # Append batch tensors to lists\n",
    "        prep_sentences.append(seq_batch)\n",
    "        sequence_lengths.append(len_batch)\n",
    "             \n",
    "    return prep_sentences, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: The man is sleeping in his bed\n",
      "Corresponding premise sequence: [4, 7, 6, 151, 5, 21, 314] \n",
      "\n",
      "Hypothesis: The man is awake\n",
      "Corresponding hypothesis sequence: [4, 7, 6, 2975]\n"
     ]
    }
   ],
   "source": [
    "# Output for example premise and hypothesis:\n",
    "sentences = ['The man is sleeping in his bed', 'The man is awake']\n",
    "prep_sentences, sequence_lengths = sent2seq(sentences, vocab)\n",
    "\n",
    "print('Premise:',sentences[0])\n",
    "print(\"Corresponding premise sequence:\", (prep_sentences[0][0].tolist()), '\\n')\n",
    "print('Hypothesis:', sentences[1])\n",
    "print(\"Corresponding hypothesis sequence:\", prep_sentences[1][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict function\n",
    "\n",
    "This function takes in prepared sentences and passes them to the desired encoder/classifier to generate a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prep_sentences, sequence_lengths, model, classifier):\n",
    "    # Extract premise and hypothesis sequence and sequence lengths\n",
    "    premise = prep_sentences[0]\n",
    "    hypothesis = prep_sentences[1]\n",
    "    premise_len = sequence_lengths[0]\n",
    "    hypothesis_len = sequence_lengths[1]\n",
    "    \n",
    "    # 'notify' model layers that we're no longer in training mode\n",
    "    model.eval()\n",
    "    torch.no_grad()\n",
    "    \n",
    "    # Pass sequences through given encoder\n",
    "    u = model(sentences=premise, sequence_lengths=premise_len)\n",
    "    v = model(sentences=hypothesis, sequence_lengths=hypothesis_len)\n",
    "\n",
    "    # Compute the joint sentence representation\n",
    "    differences = torch.abs(u-v)\n",
    "    products = torch.mul(u, v)\n",
    "    final_vectors = torch.cat((u, v, differences, products), 1)\n",
    "    \n",
    "    # Pass resulting vector through the MLP classifier\n",
    "    predictions = classifier(final_vectors)\n",
    "    \n",
    "    # Take the highest prediction, retrieve corresponding index\n",
    "    _, index = predictions.max(dim=1)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference function\n",
    "Now that we can load our models and convert our sequences into sentences, we can generate predictions for any given pair of sentences, by first passing them through the prepare function, and then feeding them through the pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentences, model_type, emb_dim, hidden_dim, epochs, bidirectional, maxpool): \n",
    "    #load vocab\n",
    "    vocab = load_vocab()\n",
    "\n",
    "    #prepare sentences by converting them into sequences with wordids.\n",
    "    prep_sentences, sequence_lengths = sent2seq(sentences, vocab)\n",
    "    \n",
    "    #load the desired model and classifier\n",
    "    model, classifier = load_model(model_type, emb_dim, hidden_dim, epochs, bidirectional, maxpool)\n",
    "    model = model.eval()\n",
    "    \n",
    "    #feed prepared sentences through model to get a prediction\n",
    "    prediction = predict(prep_sentences, sequence_lengths, model, classifier)\n",
    "\n",
    "    # \"64\" predictions is quite unncessary here, so we just take one value\n",
    "    prediction = prediction[0].item()\n",
    "    \n",
    "    label_dict = {0:'entailment', 1:'contradiction', 2:'neutral'}\n",
    "    \n",
    "    return label_dict[prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bi-LSTM with Max-Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardsnijders/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob is awake', 'Bob is sleeping'] \n",
      " True Relation: contradiction. Predicted relation:  contradiction \n",
      "\n",
      "['A car is driving on the road', 'The car is standing still'] \n",
      " True Relation: contradiction. Predicted relation:  contradiction \n",
      "\n",
      "['A man inspects the uniform of a figure in some East Asian country.', 'The man is sleeping'] \n",
      " True Relation: contradiction. Predicted relation:  contradiction \n",
      "\n",
      "['A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.'] \n",
      " True Relation: contradiction. Predicted relation:  contradiction \n",
      "\n",
      "['Bob is awake', 'Bob is lying in his bed'] \n",
      " True Relation: entailment. Predicted relation:  contradiction \n",
      "\n",
      "['A soccer game with multiple males playing.', 'Some men are playing a sport.'] \n",
      " True Relation: entailment. Predicted relation:  entailment \n",
      "\n",
      "['At the end of Pennsylvania Avenue, people began to line up for a White House tour.', 'People formed a line at the end of Pennsylvania Avenue.'] \n",
      " True Relation: entailment. Predicted relation:  entailment \n",
      "\n",
      "['Bob is awake', 'It is sunny outside'] \n",
      " True Relation: neutral. Predicted relation:  neutral \n",
      "\n",
      "['A car is driving on the road', 'It is sunny outside'] \n",
      " True Relation: neutral. Predicted relation:  neutral \n",
      "\n",
      "['A smiling costumed woman is holding an umbrella.', 'Some men are playing a sport.'] \n",
      " True Relation: neutral. Predicted relation:  contradiction \n",
      "\n",
      "['An older and younger man smiling.', 'Two men are smiling and laughing at the cats playing on the floor.'] \n",
      " True Relation: neutral. Predicted relation:  neutral \n",
      "\n",
      "['A car is driving on the road', 'The car is speeding on the highway'] \n",
      " True Relation: neutral. Predicted relation:  neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "relations = {\n",
    "    'contradiction_1' : ['Bob is awake', 'Bob is sleeping'],\n",
    "    'contradiction_2' : ['A car is driving on the road', 'The car is standing still'],\n",
    "    'contradiction_3' : ['A man inspects the uniform of a figure in some East Asian country.', 'The man is sleeping'],\n",
    "    'contradiction_4' : ['A black race car starts up in front of a crowd of people.', \n",
    "                       'A man is driving down a lonely road.'],\n",
    "    'entailment_1' : ['Bob is awake', 'Bob is lying in his bed'],\n",
    "    'entailment_3' : ['A soccer game with multiple males playing.', 'Some men are playing a sport.'],\n",
    "    'entailment_4' : ['At the end of Pennsylvania Avenue, people began to line up for a White House tour.', \n",
    "                    'People formed a line at the end of Pennsylvania Avenue.'],\n",
    "    'neutral_1' : ['Bob is awake', 'It is sunny outside'],\n",
    "    'neutral_2' : ['A car is driving on the road', 'It is sunny outside'],\n",
    "    'neutral_3' : ['A smiling costumed woman is holding an umbrella.', 'Some men are playing a sport.'],\n",
    "    'neutral_4' : ['An older and younger man smiling.', \n",
    "                 'Two men are smiling and laughing at the cats playing on the floor.'],\n",
    "    'neutral_5' : ['A car is driving on the road', 'The car is speeding on the highway']}\n",
    "for key in relations.keys():\n",
    "    print(relations[key],\n",
    "        '\\n True Relation: '+ key[:-2] + '. Predicted relation: ',\n",
    "        inference(relations[key], \n",
    "          model_type='LSTM', \n",
    "          emb_dim=300, \n",
    "          hidden_dim=2048, \n",
    "          epochs=12, \n",
    "          bidirectional=True, \n",
    "          maxpool=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results for SNLI / Sent Eval\n",
    "\n",
    "In the tables below I report the performance for the used models. I report accuracy on both the validation and test set of the SNLI natural language inference dataset. I also report the micro- and macro-accuracies on Sent Eval, as calculated from the model performance on a range of tasks, which can be observed in table 2.\n",
    "\n",
    "Model | dev (NLI) | test (NLI) | micro (SE) | macro (SE) | epoch checkpoint\n",
    "---|  --- | --- | --- | --- | ---|\n",
    "Average-Emb | 72.27 | 72.60 | 83.62 | 78.00 | 15\n",
    "Uni-LSTM | 81.70 | 81.54 | 84.06 | 81.27 | 12\n",
    "Bi-LSTM | 81.97 | 81.85| 84.02 | 81.20 | 13\n",
    "Bi-LSTM-MP | __84.70__ | __84.61__| __86.06__ | __82.88__ | 12\n",
    "\n",
    "- - - - - - - - - - - Table 1: __Performance on SNLI and SentEval.__\n",
    "\n",
    "\n",
    "\n",
    "Model | MR | CR | MPQA | SUBJ | SST2 | TREC | MRPC | SICK-E | STS14 (Pearson) | STS14 (Spearman)\n",
    "---| --- | --- | --- | --- | ---| --- | --- | --- | --- | --- |\n",
    "Average-Emb | 54.11  | 79.63     | 84.38 | 99.6 | 78.03 | 82.2 | 70.26 | 75.77 | 0.4963 | 0.5184 \n",
    "Uni-LSTM   | __71.86__ | 78.17   | 84.96 | 99.6 | 76.72 | 81.6 | 73.1 | 84.17 | 75.77 | 0.5771 | 0.5578\n",
    "Bi-LSTM    | 71.68      | 78.2     | 85.01 | 99.6 | 76.94 | 81.4 | 72.93 | 83.8 | 0.5787 | 0.5587 |\n",
    "Bi-LSTM-MP | 69.81    | __80.9__ | __85.62__ | 99.6 | __79.52__ | __87.6__ | __75.07__ | __84.92__ | __0.6383__ | __0.6238__ |\n",
    "\n",
    "- - - - -  Table 2: __Test accuracies on individual SentEval tasks.__\n",
    "\n",
    "### Performance analysis\n",
    "\n",
    "Considering the results above, we can note a couple of things. The average embedding encoder has the lowest performance, the uni and bi-LSTM encoders have comparatively better, but otherwise fairly similar performance (both approx 81.5 ~ 81.8% accuracy), and the Bi-LSTM with max-pooling outperforms the other models, with an overall  accuracy of 84.61%. Arguably, this makes sense intuitively; The average embedding encoder does not take into account word order; by assigning equal importance to each word in the sequence, it generates a comparatively simplistic sentence representation.\n",
    "\n",
    "The uni-LSTM is more sophisticated in this respect; sequential by design, the LSTM architecture has the capacity to 'remember' what it has seen in previous timesteps, which allows it to learn long distance dependencies, for instance. The bi-LSTM takes this one step further, as it processes a sentence in both a forward and backward direction, allowing it to construct a representation based on both the 'past' and the 'future' context. \n",
    "\n",
    "In this respect, I had expected the bi-LSTM to outperform the uni-LSTM slightly, but their performances are fairly similar. Nevertheless, both the uni-LSTM and bi-LSTM outperform the average embedding encoder across all relations.\n",
    "\n",
    "Finally, the bi-LSTM with max-pooling extends the bi-LSTM model by incorporating a max-pooling operation in its forward pass; here, it considers the word-level hidden states for both the forward and backward directions, such that the final sentence representation is composed of the largest values from all the word-level hidden states. This  model performs slightly better than the uni-LSTM and vanilla bi-LSTM, which seems to suggest that word-level hidden states encapsulate some additional information, compared to just the hidden states.\n",
    "\n",
    "The performance on SNLI seems to be mirrored BY the results on Sent Eval; except on task MR, the Bi-LSTM with max-pooling consistently outperforms the other encoders models. This seems to su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent Eval code\n",
    "\n",
    "Below you can see the main code that was used to generate embeddings for feeding into the Sent Eval framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, _ = load_model('AVERAGE', emb_dim=300, hidden_dim=2048, epochs=15, bidirectional=False, maxpool=False)\n",
    "\n",
    "def batcher(params, batch):\n",
    "    \"\"\"\n",
    "    params: senteval parameters.\n",
    "    batch: numpy array of text sentences (of size params.batch_size)\n",
    "    output: numpy array of sentence embeddings (of size params.batch_size)\n",
    "    \"\"\"\n",
    "    sequences, seq_lengths = sent2ids(batch)\n",
    "    embeddings = encoder(sentences=sequences, sequence_lengths=seq_lengths).detach()\n",
    "    return embeddings\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "PATH_TO_DATA = '/Users/ardsnijders/Desktop/ATCS/SentEval/data'\n",
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': False, 'kfold': 5}\n",
    "\n",
    "se = senteval.engine.SE(params, batcher)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     transfer_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC',\n",
    "#                           'MRPC', 'SICKEntailment', 'STS14']\n",
    "#     results = se.eval(transfer_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order get a better idea of where the models perform well, I loaded the test set in DataFrame and performed inference for all models. This allows us to see more easily how well the models were able to predict specific relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_ae7e3772_827e_11ea_94a4_acbc32902497row0_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row0_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row1_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row1_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row2_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row2_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row3_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row3_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row4_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_ae7e3772_827e_11ea_94a4_acbc32902497row4_col1 {\n",
       "            width:  300px;\n",
       "        }</style><table id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Premise</th>        <th class=\"col_heading level0 col1\" >Hypothesis</th>        <th class=\"col_heading level0 col2\" >gold_label</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row0_col0\" class=\"data row0 col0\" >This church choir sings to the masses as they sing joyous songs from the book at a church.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row0_col1\" class=\"data row0 col1\" >The church has cracks in the ceiling.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row0_col2\" class=\"data row0 col2\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row1_col0\" class=\"data row1 col0\" >This church choir sings to the masses as they sing joyous songs from the book at a church.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row1_col1\" class=\"data row1 col1\" >The church is filled with song.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row1_col2\" class=\"data row1 col2\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row2_col0\" class=\"data row2 col0\" >This church choir sings to the masses as they sing joyous songs from the book at a church.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row2_col1\" class=\"data row2 col1\" >A choir singing at a baseball game.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row2_col2\" class=\"data row2 col2\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row3_col0\" class=\"data row3 col0\" >A woman with a green headscarf, blue shirt and a very big grin.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row3_col1\" class=\"data row3 col1\" >The woman is young.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row3_col2\" class=\"data row3 col2\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row4_col0\" class=\"data row4 col0\" >A woman with a green headscarf, blue shirt and a very big grin.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row4_col1\" class=\"data row4 col1\" >The woman is very happy.</td>\n",
       "                        <td id=\"T_ae7e3772_827e_11ea_94a4_acbc32902497row4_col2\" class=\"data row4 col2\" >entailment</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a415eb150>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "df = pd.read_csv('snli_1.0_test.txt', sep='\t')\n",
    "df = df[['sentence1', 'sentence2', 'gold_label']]\n",
    "df.rename(columns={'sentence1': 'Premise', 'sentence2': 'Hypothesis'}, inplace=True)\n",
    "# delete the entries where the gold label == '-'\n",
    "df = df[df['gold_label'] != '-']\n",
    "df_ex = df.head(5)\n",
    "df_ex.style.set_properties(subset=['Premise', 'Hypothesis'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of omitted rows: 176\n"
     ]
    }
   ],
   "source": [
    "print('Number of omitted rows:', 10000-len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions for generating predictions from DataFrame\n",
    "\n",
    "### In a nutshell: \n",
    "\n",
    "*create_batches* creates iterables using both the premise and hypothesis columns in the dataframe.\n",
    "\n",
    "Then, *sent2ids* processes these batches, converts sentences to sequences of ids and records their lengths. This function is also used by the *batcher* function which is used for SentEval.\n",
    "\n",
    "Next, *get_predictions* uses batches of sequences and lengths to generate predictions for the desired model.\n",
    "\n",
    "Finally, *calc_accuracy* can be used to compute the accuracy by feeding in a dataframe of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_size, column, dataframe):\n",
    "    val_iter = []\n",
    "    batch = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        pair = [row[column]]\n",
    "        batch.append(pair)\n",
    "        if (index+1) % batch_size == 0:\n",
    "            val_iter.append(batch)\n",
    "            batch = []\n",
    "    return val_iter\n",
    "\n",
    "def sent2ids(batch):   \n",
    "    # Tokenize & lowercase\n",
    "    batch = [sent if sent != [] else ['.'] for sent in batch]\n",
    "    batch = [' '.join(sentence) for sentence in batch]\n",
    "    batch = [nltk.tokenize.word_tokenize(sentence.lower()) for sentence in batch]\n",
    "    \n",
    "    # Record the sentence lengths\n",
    "    seq_lengths = [len(sent) for sent in batch] \n",
    "    # Record max seq_length\n",
    "    batch_size = len(seq_lengths)\n",
    "    max_len = max(seq_lengths)\n",
    "    \n",
    "    # Create a tensor with ones of shape batch_size, max_seq_len\n",
    "    sequences = torch.ones((batch_size, max_len),dtype=torch.long)\n",
    "    # Create a tensor with sequence lengths\n",
    "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
    "    \n",
    "    # Convert to sequences with ids, populate sequences tensor\n",
    "    for i, sentence in enumerate(batch):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in vocab:\n",
    "                sequences[i][j] = vocab[word]\n",
    "            else:\n",
    "                sequences[i][j] = vocab['<unk>']              \n",
    "    # Delete rows where seq_length = 0, as these are empty sentences\n",
    "    sequences = sequences[seq_lengths>0]\n",
    "    \n",
    "    # Adjust the seq_lengths accordingly\n",
    "    seq_lengths = seq_lengths[seq_lengths>0]\n",
    "    \n",
    "    return sequences, seq_lengths\n",
    "\n",
    "def get_predictions(premise_iter, hypothesis_iter, model_type, emb_dim, hidden_dim, epochs, bidirectional, maxpool):\n",
    "    \n",
    "    vocab = load_vocab()\n",
    "    result = []\n",
    "    for i in range(len(premise_iter)):  \n",
    "        #load the desired model and classifier\n",
    "        model, classifier = load_model(model_type, emb_dim, hidden_dim, epochs, bidirectional, maxpool)\n",
    "        \n",
    "        premises, premise_lens = sent2ids(premise_iter[i]) \n",
    "        hypotheses, hypothesis_lens = sent2ids(hypothesis_iter[i])\n",
    "        \n",
    "        prep_sentences = [premises, hypotheses]\n",
    "        sequence_lengths = [premise_lens, hypothesis_lens]\n",
    "        \n",
    "        predictions = predict(prep_sentences=prep_sentences,\n",
    "                              sequence_lengths=sequence_lengths, \n",
    "                              model=model, classifier=classifier)\n",
    "        \n",
    "        label_dict = {0:'entailment', 1:'contradiction', 2:'neutral'}\n",
    "        \n",
    "        for prediction in predictions:\n",
    "            result.append( label_dict[prediction.item()] )\n",
    "        \n",
    "#         print('{} out of {} batches for {} with bidir: {} and maxpool: {} processed'.format(i+1, len(premise_iter), \n",
    "                                                                                            #model_type, bidirectional, maxpool))      \n",
    "    return result\n",
    "\n",
    "def calc_accuracy(model, dataframe):\n",
    "    \n",
    "    correct = len( dataframe[ dataframe[model]==dataframe['gold_label'] ] )\n",
    "    acc = round( 100 * (correct / len(dataframe)), 2)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions from DataFrame\n",
    "\n",
    "In the cells below, I create batch objects from the premise and hypothesis columns in the dataframe containing the test set examples. I then pass these on to get_predictions to predict the relations between sentences, for each sentence encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch objects with size 100\n",
    "premise_iter = create_batches(100, 'Premise', df)\n",
    "hypothesis_iter = create_batches(100, 'Hypothesis',df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below were ran to get predictions using the Average Embedding encoder, Uni-LSTM, Bi-LSTM, and Bi-LSTM-MP, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "average    = get_predictions(premise_iter, hypothesis_iter,\n",
    "                          model_type='AVERAGE', emb_dim=300, \n",
    "                          hidden_dim=2048, epochs=15, \n",
    "                          bidirectional=False, maxpool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_lstm   = get_predictions(premise_iter, hypothesis_iter,\n",
    "                          model_type='LSTM', emb_dim=300, \n",
    "                          hidden_dim=2048, epochs=12, \n",
    "                          bidirectional=False, maxpool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm    = get_predictions(premise_iter, hypothesis_iter,\n",
    "                          model_type='LSTM', emb_dim=300, \n",
    "                          hidden_dim=2048, epochs=13, \n",
    "                          bidirectional=True, maxpool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_mp = get_predictions(premise_iter, hypothesis_iter, \n",
    "                          model_type='LSTM', \n",
    "                          emb_dim=300, hidden_dim=2048, \n",
    "                          epochs=12, bidirectional=True, maxpool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the previously computed relation predictions to separate columns in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVERAGE'] = average\n",
    "df['UNI_LSTM'] = uni_lstm\n",
    "df['BI_LSTM'] = bi_lstm\n",
    "df['BI_LSTM_MP'] = bi_lstm_mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating separate dataframes for each relation to allow calculation of per-relation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "contradictions = df[df['gold_label']=='contradiction']\n",
    "entailments = df[df['gold_label']=='entailment']\n",
    "neutrals = df[df['gold_label']=='neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing classification accuracy for each model, for each relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples in test set: 9824\n",
      "\n",
      "Overall classification accuracy using AVERAGE encoder: 72.6%\n",
      "Accuracy using AVERAGE encoder on predicting relation: contradiction: 70.37%\n",
      "Accuracy using AVERAGE encoder on predicting relation: entailment: 78.95%\n",
      "Accuracy using AVERAGE encoder on predicting relation: neutral: 68.19%\n",
      "\n",
      "\n",
      "Overall classification accuracy using UNI_LSTM encoder: 81.54%\n",
      "Accuracy using UNI_LSTM encoder on predicting relation: contradiction: 84.37%\n",
      "Accuracy using UNI_LSTM encoder on predicting relation: entailment: 82.84%\n",
      "Accuracy using UNI_LSTM encoder on predicting relation: neutral: 77.32%\n",
      "\n",
      "\n",
      "Overall classification accuracy using BI_LSTM encoder: 81.85%\n",
      "Accuracy using BI_LSTM encoder on predicting relation: contradiction: 83.78%\n",
      "Accuracy using BI_LSTM encoder on predicting relation: entailment: 83.49%\n",
      "Accuracy using BI_LSTM encoder on predicting relation: neutral: 78.19%\n",
      "\n",
      "\n",
      "Overall classification accuracy using BI_LSTM_MP encoder: 84.61%\n",
      "Accuracy using BI_LSTM_MP encoder on predicting relation: contradiction: 86.31%\n",
      "Accuracy using BI_LSTM_MP encoder on predicting relation: entailment: 86.7%\n",
      "Accuracy using BI_LSTM_MP encoder on predicting relation: neutral: 80.71%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total number of examples in test set: ' + str(len(df)) +'\\n')\n",
    "\n",
    "# Compute classification accuracy for each relation, for each model\n",
    "models = ['AVERAGE', 'UNI_LSTM', 'BI_LSTM', 'BI_LSTM_MP']\n",
    "categories = [contradictions, entailments, neutrals]\n",
    "for model in models:\n",
    "    print('Overall classification accuracy using '+model+' encoder: '+str(calc_accuracy(model,df)) +'%')\n",
    "    for category in categories:\n",
    "        print('Accuracy using ' + \n",
    "              str(model) + ' encoder on predicting relation: ' + \n",
    "              category.gold_label.iloc[0] + \n",
    "              ': ' + str(calc_accuracy(model, category))\n",
    "              + '%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation-wise results\n",
    "### Entailment\n",
    "Most models seem to perform the best when predicting 'entailment'; notably, the average embedding encoder has an accuracy of almost 80% for this category. In case of the uni- and bi-LSTMs, there doesn't seem to be a clear difference in performance between 'entailment' and 'contradiction'. Possibly, a reason why predicting this relation is \"easy\" is because most of the entailment problems in the data are comprised of two sentences which are virtually identical, semantically speaking. Moreover, the hypothesis in these cases is usually a very 'succinct' version of the premise. For instance, consider the following sentence pair:\n",
    "\n",
    "Premise: *An old man with a package poses in front of an advertisement*\n",
    "\n",
    "Hypothesis: *A man poses in front of an ad*\n",
    "\n",
    "In this case, the hypothesis does not present any information which is not already explicitly mentioned in the premise, which might make it easy for models to infer that the relation is entailment. Most of the entailment problems in the data seem to be formulated like this; Long, detailed premises with short, succint hypotheses.\n",
    "\n",
    "It seems that the models struggle more when the hypothesis contains information which can be *inferred* from the premise using world knowledge, but is not stated in the premise explicitly. Consider the following sentence pair:\n",
    "\n",
    "Premise: *A land rover is being driven across a river*\n",
    "\n",
    "Hypothesis: *A land rover is splashing water as it crosses a river*\n",
    "\n",
    "Anyone who has knowledge about 'water' and its behaviour in the real world knows that it will 'splash' when disturbing it - in this case, by driving through it. For machines however, such deductions are not necessarily as straightforward; rather, they might 'perceive' the phenomenon of 'splashing' to be something that cannot be consistent with the premise phrase 'driven across a river', (even though arguably, the words are semantically close) - resultantly, the models all (falsely) predict 'entailment'.\n",
    "\n",
    "In this respect, the fact that all models perform the best on entailment problems might be a little misleading, in the sense that most of the entailment problems (I say most, though I obivously haven't looked at all 180k+ thousand of them - I just base this on scanning the dataframes) are formulated rather simplistically; when faced with more difficult/ambiguous entailment problems, the models will possibly struggle more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_825d5658_8280_11ea_94a4_acbc32902497row0_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row0_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row1_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row1_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row2_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row2_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row3_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row3_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row4_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row4_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row5_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row5_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row6_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row6_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row7_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row7_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row8_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row8_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row9_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_825d5658_8280_11ea_94a4_acbc32902497row9_col1 {\n",
       "            width:  300px;\n",
       "        }</style><table id=\"T_825d5658_8280_11ea_94a4_acbc32902497\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Premise</th>        <th class=\"col_heading level0 col1\" >Hypothesis</th>        <th class=\"col_heading level0 col2\" >gold_label</th>        <th class=\"col_heading level0 col3\" >AVERAGE</th>        <th class=\"col_heading level0 col4\" >UNI_LSTM</th>        <th class=\"col_heading level0 col5\" >BI_LSTM</th>        <th class=\"col_heading level0 col6\" >BI_LSTM_MP</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col0\" class=\"data row0 col0\" >This church choir sings to the masses as they sing joyous songs from the book at a church.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col1\" class=\"data row0 col1\" >The church is filled with song.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col2\" class=\"data row0 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col3\" class=\"data row0 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col4\" class=\"data row0 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col5\" class=\"data row0 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row0_col6\" class=\"data row0 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col0\" class=\"data row1 col0\" >A woman with a green headscarf, blue shirt and a very big grin.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col1\" class=\"data row1 col1\" >The woman is very happy.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col2\" class=\"data row1 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col3\" class=\"data row1 col3\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col4\" class=\"data row1 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col5\" class=\"data row1 col5\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row1_col6\" class=\"data row1 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row2\" class=\"row_heading level0 row2\" >6</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col0\" class=\"data row2 col0\" >An old man with a package poses in front of an advertisement.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col1\" class=\"data row2 col1\" >A man poses in front of an ad.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col2\" class=\"data row2 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col3\" class=\"data row2 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col4\" class=\"data row2 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col5\" class=\"data row2 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row2_col6\" class=\"data row2 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row3\" class=\"row_heading level0 row3\" >10</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col0\" class=\"data row3 col0\" >A statue at a museum that no seems to be looking at.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col1\" class=\"data row3 col1\" >There is a statue that not many people seem to be interested in.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col2\" class=\"data row3 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col3\" class=\"data row3 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col4\" class=\"data row3 col4\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col5\" class=\"data row3 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row3_col6\" class=\"data row3 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row4\" class=\"row_heading level0 row4\" >12</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col0\" class=\"data row4 col0\" >A land rover is being driven across a river.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col1\" class=\"data row4 col1\" >A Land Rover is splashing water as it crosses a river.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col2\" class=\"data row4 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col3\" class=\"data row4 col3\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col4\" class=\"data row4 col4\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col5\" class=\"data row4 col5\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row4_col6\" class=\"data row4 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row5\" class=\"row_heading level0 row5\" >13</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col0\" class=\"data row5 col0\" >A land rover is being driven across a river.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col1\" class=\"data row5 col1\" >A vehicle is crossing a river.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col2\" class=\"data row5 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col3\" class=\"data row5 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col4\" class=\"data row5 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col5\" class=\"data row5 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row5_col6\" class=\"data row5 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row6\" class=\"row_heading level0 row6\" >16</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col0\" class=\"data row6 col0\" >A man playing an electric guitar on stage.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col1\" class=\"data row6 col1\" >A man playing guitar on stage.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col2\" class=\"data row6 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col3\" class=\"data row6 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col4\" class=\"data row6 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col5\" class=\"data row6 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row6_col6\" class=\"data row6 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row7\" class=\"row_heading level0 row7\" >18</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col0\" class=\"data row7 col0\" >A blond-haired doctor and her African american assistant looking threw new medical manuals.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col1\" class=\"data row7 col1\" >A doctor is looking at a book</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col2\" class=\"data row7 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col3\" class=\"data row7 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col4\" class=\"data row7 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col5\" class=\"data row7 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row7_col6\" class=\"data row7 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row8\" class=\"row_heading level0 row8\" >22</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col0\" class=\"data row8 col0\" >One tan girl with a wool hat is running and leaning over an object, while another person in a wool hat is sitting on the ground.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col1\" class=\"data row8 col1\" >A tan girl runs leans over an object</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col2\" class=\"data row8 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col3\" class=\"data row8 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col4\" class=\"data row8 col4\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col5\" class=\"data row8 col5\" >neutral</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row8_col6\" class=\"data row8 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_825d5658_8280_11ea_94a4_acbc32902497level0_row9\" class=\"row_heading level0 row9\" >26</th>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col0\" class=\"data row9 col0\" >A young family enjoys feeling ocean waves lap at their feet.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col1\" class=\"data row9 col1\" >A family is at the beach.</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col2\" class=\"data row9 col2\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col3\" class=\"data row9 col3\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col4\" class=\"data row9 col4\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col5\" class=\"data row9 col5\" >entailment</td>\n",
       "                        <td id=\"T_825d5658_8280_11ea_94a4_acbc32902497row9_col6\" class=\"data row9 col6\" >entailment</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a41c185d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entail = df[(df.gold_label=='entailment')].head(10)# & (df.gold_label != df.AVERAGE)]\n",
    "df_entail.style.set_properties(subset=['Premise', 'Hypothesis'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contradiction\n",
    "\n",
    "All models seem to do slightly worse when predicting 'contradiction', save for the uni- and bi-LSTMs, which as mentioned, have comparable accuracies for this relation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col1 {\n",
       "            width:  300px;\n",
       "        }</style><table id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Premise</th>        <th class=\"col_heading level0 col1\" >Hypothesis</th>        <th class=\"col_heading level0 col2\" >gold_label</th>        <th class=\"col_heading level0 col3\" >AVERAGE</th>        <th class=\"col_heading level0 col4\" >UNI_LSTM</th>        <th class=\"col_heading level0 col5\" >BI_LSTM</th>        <th class=\"col_heading level0 col6\" >BI_LSTM_MP</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col0\" class=\"data row0 col0\" >This church choir sings to the masses as they sing joyous songs from the book at a church.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col1\" class=\"data row0 col1\" >A choir singing at a baseball game.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col2\" class=\"data row0 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col3\" class=\"data row0 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col4\" class=\"data row0 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col5\" class=\"data row0 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row0_col6\" class=\"data row0 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row1\" class=\"row_heading level0 row1\" >5</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col0\" class=\"data row1 col0\" >A woman with a green headscarf, blue shirt and a very big grin.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col1\" class=\"data row1 col1\" >The woman has been shot.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col2\" class=\"data row1 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col3\" class=\"data row1 col3\" >entailment</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col4\" class=\"data row1 col4\" >neutral</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col5\" class=\"data row1 col5\" >neutral</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row1_col6\" class=\"data row1 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row2\" class=\"row_heading level0 row2\" >8</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col0\" class=\"data row2 col0\" >An old man with a package poses in front of an advertisement.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col1\" class=\"data row2 col1\" >A man walks by an ad.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col2\" class=\"data row2 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col3\" class=\"data row2 col3\" >entailment</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col4\" class=\"data row2 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col5\" class=\"data row2 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row2_col6\" class=\"data row2 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row3\" class=\"row_heading level0 row3\" >11</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col0\" class=\"data row3 col0\" >A statue at a museum that no seems to be looking at.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col1\" class=\"data row3 col1\" >Tons of people are gathered around the statue.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col2\" class=\"data row3 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col3\" class=\"data row3 col3\" >neutral</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col4\" class=\"data row3 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col5\" class=\"data row3 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row3_col6\" class=\"data row3 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row4\" class=\"row_heading level0 row4\" >14</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col0\" class=\"data row4 col0\" >A land rover is being driven across a river.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col1\" class=\"data row4 col1\" >A sedan is stuck in the middle of a river.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col2\" class=\"data row4 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col3\" class=\"data row4 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col4\" class=\"data row4 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col5\" class=\"data row4 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row4_col6\" class=\"data row4 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row5\" class=\"row_heading level0 row5\" >15</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col0\" class=\"data row5 col0\" >A man playing an electric guitar on stage.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col1\" class=\"data row5 col1\" >A man playing banjo on the floor.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col2\" class=\"data row5 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col3\" class=\"data row5 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col4\" class=\"data row5 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col5\" class=\"data row5 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row5_col6\" class=\"data row5 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row6\" class=\"row_heading level0 row6\" >19</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col0\" class=\"data row6 col0\" >A blond-haired doctor and her African american assistant looking threw new medical manuals.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col1\" class=\"data row6 col1\" >A man is eating pb and j</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col2\" class=\"data row6 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col3\" class=\"data row6 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col4\" class=\"data row6 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col5\" class=\"data row6 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row6_col6\" class=\"data row6 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row7\" class=\"row_heading level0 row7\" >21</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col0\" class=\"data row7 col0\" >One tan girl with a wool hat is running and leaning over an object, while another person in a wool hat is sitting on the ground.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col1\" class=\"data row7 col1\" >A boy runs into a wall</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col2\" class=\"data row7 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col3\" class=\"data row7 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col4\" class=\"data row7 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col5\" class=\"data row7 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row7_col6\" class=\"data row7 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row8\" class=\"row_heading level0 row8\" >25</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col0\" class=\"data row8 col0\" >A young family enjoys feeling ocean waves lap at their feet.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col1\" class=\"data row8 col1\" >A family is out at a restaurant.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col2\" class=\"data row8 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col3\" class=\"data row8 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col4\" class=\"data row8 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col5\" class=\"data row8 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row8_col6\" class=\"data row8 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497level0_row9\" class=\"row_heading level0 row9\" >29</th>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col0\" class=\"data row9 col0\" >A couple walk hand in hand down a street.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col1\" class=\"data row9 col1\" >A couple is sitting on a bench.</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col2\" class=\"data row9 col2\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col3\" class=\"data row9 col3\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col4\" class=\"data row9 col4\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col5\" class=\"data row9 col5\" >contradiction</td>\n",
       "                        <td id=\"T_38fd41ca_8286_11ea_94a4_acbc32902497row9_col6\" class=\"data row9 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a34fb5950>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contra = df[(df.gold_label=='contradiction')].head(10)# & (df.gold_label != df.AVERAGE)]\n",
    "df_contra.style.set_properties(subset=['Premise', 'Hypothesis'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral\n",
    "\n",
    "Loosely speaking, it appears that 'neutral' makes for the most difficult category. A possible reason for this could be that intuitively, this task is also the most difficult in general - In my opinion, there's comparatively more ambiguity involved, especially for sentences where it holds that \"*X* being true in both the premise and hypothesis, need not necessarily entail *Y* for the hypothesis, even if *Y* does not contradict *X*\". I suppose getting these right requires more 'inference' ability; in this respect some of them are almost trick-questions. For instance, consider the following pair of sentences, from the dev set:\n",
    "\n",
    "*Premise: A group of people sit on benches at a park outside a building.\t\n",
    "Hypothesis: The people sit together.*\n",
    "\n",
    "All models predict 'entailment' for this relation, but it is labelled 'neutral'; arguably, a group of people sitting on benches does not necessarily entail that they are sitting *together*. Possibly, the models make a connection between the words 'group' and 'together', which one could argue are semantically closely related; in this respect, the hypothesis of sitting together might be entailed by the premise that there is a group of people, explaining why all models predict 'entailment'.\n",
    "\n",
    "Some of the other 'neutral' relations are more straightforward:\n",
    "\n",
    "*Premise: A man sits in a chair on the sidewalk in front of a huge display of brightly colored artwork.\t\n",
    "Hypothesis: The man was selling the artwork*\n",
    "\n",
    "Possibly, because the word *selling* is somewhat semantically distant from most of the words in the premise, all models are able to quite easily determine that the hypothesis cannot be entailed from the premise; furthermore, there are no words in the hypothesis that would indicate a contradiction. All models correctly predicted 'neutral' for this pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col1 {\n",
       "            width:  300px;\n",
       "        }</style><table id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Premise</th>        <th class=\"col_heading level0 col1\" >Hypothesis</th>        <th class=\"col_heading level0 col2\" >gold_label</th>        <th class=\"col_heading level0 col3\" >AVERAGE</th>        <th class=\"col_heading level0 col4\" >UNI_LSTM</th>        <th class=\"col_heading level0 col5\" >BI_LSTM</th>        <th class=\"col_heading level0 col6\" >BI_LSTM_MP</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col0\" class=\"data row0 col0\" >This church choir sings to the masses as they sing joyous songs from the book at a church.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col1\" class=\"data row0 col1\" >The church has cracks in the ceiling.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col2\" class=\"data row0 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col3\" class=\"data row0 col3\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col4\" class=\"data row0 col4\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col5\" class=\"data row0 col5\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row0_col6\" class=\"data row0 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row1\" class=\"row_heading level0 row1\" >3</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col0\" class=\"data row1 col0\" >A woman with a green headscarf, blue shirt and a very big grin.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col1\" class=\"data row1 col1\" >The woman is young.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col2\" class=\"data row1 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col3\" class=\"data row1 col3\" >entailment</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col4\" class=\"data row1 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col5\" class=\"data row1 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row1_col6\" class=\"data row1 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row2\" class=\"row_heading level0 row2\" >7</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col0\" class=\"data row2 col0\" >An old man with a package poses in front of an advertisement.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col1\" class=\"data row2 col1\" >A man poses in front of an ad for beer.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col2\" class=\"data row2 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col3\" class=\"data row2 col3\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col4\" class=\"data row2 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col5\" class=\"data row2 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row2_col6\" class=\"data row2 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row3\" class=\"row_heading level0 row3\" >9</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col0\" class=\"data row3 col0\" >A statue at a museum that no seems to be looking at.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col1\" class=\"data row3 col1\" >The statue is offensive and people are mad that it is on display.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col2\" class=\"data row3 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col3\" class=\"data row3 col3\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col4\" class=\"data row3 col4\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col5\" class=\"data row3 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row3_col6\" class=\"data row3 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row4\" class=\"row_heading level0 row4\" >17</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col0\" class=\"data row4 col0\" >A man playing an electric guitar on stage.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col1\" class=\"data row4 col1\" >A man is performing for cash.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col2\" class=\"data row4 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col3\" class=\"data row4 col3\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col4\" class=\"data row4 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col5\" class=\"data row4 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row4_col6\" class=\"data row4 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row5\" class=\"row_heading level0 row5\" >20</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col0\" class=\"data row5 col0\" >A blond-haired doctor and her African american assistant looking threw new medical manuals.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col1\" class=\"data row5 col1\" >A doctor is studying</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col2\" class=\"data row5 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col3\" class=\"data row5 col3\" >entailment</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col4\" class=\"data row5 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col5\" class=\"data row5 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row5_col6\" class=\"data row5 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row6\" class=\"row_heading level0 row6\" >23</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col0\" class=\"data row6 col0\" >One tan girl with a wool hat is running and leaning over an object, while another person in a wool hat is sitting on the ground.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col1\" class=\"data row6 col1\" >A man watches his daughter leap</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col2\" class=\"data row6 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col3\" class=\"data row6 col3\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col4\" class=\"data row6 col4\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col5\" class=\"data row6 col5\" >contradiction</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row6_col6\" class=\"data row6 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row7\" class=\"row_heading level0 row7\" >24</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col0\" class=\"data row7 col0\" >A young family enjoys feeling ocean waves lap at their feet.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col1\" class=\"data row7 col1\" >A young man and woman take their child to the beach for the first time.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col2\" class=\"data row7 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col3\" class=\"data row7 col3\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col4\" class=\"data row7 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col5\" class=\"data row7 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row7_col6\" class=\"data row7 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row8\" class=\"row_heading level0 row8\" >28</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col0\" class=\"data row8 col0\" >A couple walk hand in hand down a street.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col1\" class=\"data row8 col1\" >The couple is married.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col2\" class=\"data row8 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col3\" class=\"data row8 col3\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col4\" class=\"data row8 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col5\" class=\"data row8 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row8_col6\" class=\"data row8 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497level0_row9\" class=\"row_heading level0 row9\" >34</th>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col0\" class=\"data row9 col0\" >A man reads the paper in a bar with green lighting.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col1\" class=\"data row9 col1\" >The man is reading the sportspage.</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col2\" class=\"data row9 col2\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col3\" class=\"data row9 col3\" >entailment</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col4\" class=\"data row9 col4\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col5\" class=\"data row9 col5\" >neutral</td>\n",
       "                        <td id=\"T_0ec2651e_8242_11ea_94a4_acbc32902497row9_col6\" class=\"data row9 col6\" >neutral</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a381fca50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neutral = df[(df.gold_label=='neutral')].head(10)# & (df.gold_label != df.AVERAGE)]\n",
    "df_neutral.style.set_properties(subset=['Premise', 'Hypothesis'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Mini Perturbation Study: How much does word order really matter?\n",
    "\n",
    "I wanted to get a better idea on what types of information the different encoders depend on most when generating sentence embeddings for relation classification. I do this by introducing a perturbation in the data *at test time*: shuffling words within sentences, such that the sentences no longer have any meaningful sense of word order. Namely, I made a copy of the original DataFrame, shuffled the words in the sentences, and ran inferences for all 4 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardsnijders/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "2 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "3 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "4 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "5 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "6 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "7 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "8 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "9 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "10 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "11 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "12 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "13 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "14 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "15 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "16 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "17 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "18 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "19 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "20 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "21 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "22 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "23 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "24 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "25 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "26 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "27 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "28 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "29 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "30 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "31 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "32 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "33 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "34 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "35 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "36 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "37 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "38 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "39 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "40 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "41 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "42 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "43 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "44 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "45 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "46 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "47 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "48 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "49 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "50 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "51 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "52 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "53 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "54 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "55 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "56 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "57 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "58 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "59 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "60 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "61 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "62 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "63 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "64 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "65 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "66 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "67 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "68 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "69 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "70 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "71 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "72 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "73 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "74 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "75 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "76 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "77 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "78 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "79 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "80 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "81 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "82 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "83 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "84 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "85 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "86 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "87 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "88 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "89 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "90 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "91 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "92 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "93 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "94 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "95 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "96 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "97 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "98 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "99 out of 99 batches for AVERAGE with bidir: False and maxpool: False processed\n",
      "1 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "2 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "3 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "4 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "5 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "6 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "7 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "8 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "9 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "10 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "11 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "12 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "13 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "14 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "15 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "16 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "17 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "18 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "19 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "20 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "21 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "22 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "23 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "24 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "25 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "26 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "27 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "28 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "29 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "30 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "31 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "32 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "33 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "34 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "35 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "36 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "37 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "38 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "39 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "40 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "41 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "42 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "43 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "44 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "45 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "46 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "47 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "48 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "49 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "50 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "51 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "52 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "53 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "54 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "55 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "56 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "57 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "58 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "59 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "60 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "61 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "62 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "63 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "64 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "65 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "66 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "67 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "68 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "69 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "70 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "71 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "72 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "73 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "74 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "75 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "76 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "77 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "78 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "79 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "80 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "81 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "82 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "83 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "84 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "85 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "86 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "87 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "88 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "89 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "90 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "91 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "92 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "93 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "94 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "95 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "96 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "97 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "98 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "99 out of 99 batches for LSTM with bidir: True and maxpool: False processed\n",
      "1 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "2 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "3 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "4 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "5 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "6 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "7 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "8 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "9 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "10 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "11 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "12 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "13 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "14 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "15 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "16 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "17 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "18 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "19 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "20 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "21 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "22 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "23 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "24 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "25 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "26 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "27 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "28 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "29 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "30 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "31 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "32 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "33 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "34 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "35 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "36 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "37 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "38 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "39 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "40 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "41 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "42 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "43 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "44 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "45 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "46 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "47 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "48 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "49 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "50 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "51 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "52 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "53 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "54 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "55 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "56 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "57 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "58 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "59 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "60 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "61 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "62 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "63 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "64 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "65 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "66 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "67 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "68 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "69 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "70 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "71 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "72 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "73 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "74 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "75 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "76 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "77 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "78 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "79 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "80 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "81 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "82 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "83 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "84 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "85 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "86 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "87 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "88 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "89 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "90 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "91 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "92 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "93 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "94 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "95 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "96 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "97 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "98 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "99 out of 99 batches for LSTM with bidir: False and maxpool: False processed\n",
      "1 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "2 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "3 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "4 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "5 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "6 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "7 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "8 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "9 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "10 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "11 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "12 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "13 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "14 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "15 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "16 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "17 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "18 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "19 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "20 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "21 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "22 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "23 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "24 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "25 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "26 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "27 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "28 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "29 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "30 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "31 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "32 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "33 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "34 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "35 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "36 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "37 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "38 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "39 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "40 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "41 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "42 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "43 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "44 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "45 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "46 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "47 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "48 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "49 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "50 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "51 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "52 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "53 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "54 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "55 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "56 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "57 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "58 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "59 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "60 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "61 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "62 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "63 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "64 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "65 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "66 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "67 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "68 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "69 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "70 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "71 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "72 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "73 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "74 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "75 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "76 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "77 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "78 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "79 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "80 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "81 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "82 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "83 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "84 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "85 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "86 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "87 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "88 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "89 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "90 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "91 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "92 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "93 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "94 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "95 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "96 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "97 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "98 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n",
      "99 out of 99 batches for LSTM with bidir: True and maxpool: True processed\n"
     ]
    }
   ],
   "source": [
    "# Copy DataFrame\n",
    "df2 = df[['Premise', 'Hypothesis', 'gold_label']]\n",
    "\n",
    "def scramble(sentence):\n",
    "    shuffled_sent = sentence.split()\n",
    "    random.shuffle(shuffled_sent)\n",
    "    return ' '.join(shuffled_sent)\n",
    "\n",
    "columns = ['Premise', 'Hypothesis']\n",
    "\n",
    "# Shuffle sentences\n",
    "for column in columns:\n",
    "    df2[column] = df2[column].apply(lambda x: scramble(x))\n",
    "    \n",
    "# Create batch objects with size 100\n",
    "premise_iter = create_batches(100, 'Premise', df2)\n",
    "hypothesis_iter = create_batches(100, 'Hypothesis', df2)\n",
    "\n",
    "# Generate predictions for every encoder\n",
    "average_2    = get_predictions(premise_iter, hypothesis_iter,\n",
    "                          model_type='AVERAGE', emb_dim=300, \n",
    "                          hidden_dim=2048, epochs=15, \n",
    "                          bidirectional=False, maxpool=False)\n",
    "\n",
    "bi_lstm_2    = get_predictions(premise_iter, hypothesis_iter,\n",
    "                          model_type='LSTM', emb_dim=300, \n",
    "                          hidden_dim=2048, epochs=13, \n",
    "                          bidirectional=True, maxpool=False)\n",
    "\n",
    "uni_lstm_2   = get_predictions(premise_iter, hypothesis_iter,\n",
    "                          model_type='LSTM', emb_dim=300, \n",
    "                          hidden_dim=2048, epochs=12, \n",
    "                          bidirectional=False, maxpool=False)\n",
    "\n",
    "bi_lstm_mp_2 = get_predictions(premise_iter, hypothesis_iter, \n",
    "                          model_type='LSTM', \n",
    "                          emb_dim=300, hidden_dim=2048, \n",
    "                          epochs=12, bidirectional=True, maxpool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ardsnijders/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/ardsnijders/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ardsnijders/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Add predictions for each model by creating new columns in df2\n",
    "df2['AVERAGE'] = average_2\n",
    "df2['UNI_LSTM'] = uni_lstm_2\n",
    "df2['BI_LSTM'] = bi_lstm_2\n",
    "df2['BI_LSTM_MP'] = bi_lstm_mp_2\n",
    "\n",
    "# Create sub-DataFrames for each relation\n",
    "contradictions = df2[df2['gold_label']=='contradiction']\n",
    "entailments = df2[df2['gold_label']=='entailment']\n",
    "neutrals = df2[df2['gold_label']=='neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_1e49258e_8271_11ea_94a4_acbc32902497row0_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row0_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row1_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row1_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row2_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row2_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row3_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row3_col1 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row4_col0 {\n",
       "            width:  300px;\n",
       "        }    #T_1e49258e_8271_11ea_94a4_acbc32902497row4_col1 {\n",
       "            width:  300px;\n",
       "        }</style><table id=\"T_1e49258e_8271_11ea_94a4_acbc32902497\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Premise</th>        <th class=\"col_heading level0 col1\" >Hypothesis</th>        <th class=\"col_heading level0 col2\" >gold_label</th>        <th class=\"col_heading level0 col3\" >AVERAGE</th>        <th class=\"col_heading level0 col4\" >UNI_LSTM</th>        <th class=\"col_heading level0 col5\" >BI_LSTM</th>        <th class=\"col_heading level0 col6\" >BI_LSTM_MP</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1e49258e_8271_11ea_94a4_acbc32902497level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col0\" class=\"data row0 col0\" >the as sing joyous church the to a sings they songs from choir at book church. masses This</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col1\" class=\"data row0 col1\" >cracks church the ceiling. has The in</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col2\" class=\"data row0 col2\" >neutral</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col3\" class=\"data row0 col3\" >contradiction</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col4\" class=\"data row0 col4\" >contradiction</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col5\" class=\"data row0 col5\" >neutral</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row0_col6\" class=\"data row0 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e49258e_8271_11ea_94a4_acbc32902497level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col0\" class=\"data row1 col0\" >from the book masses songs sings a sing church. as choir at they church This the joyous to</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col1\" class=\"data row1 col1\" >church song. filled is with The</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col2\" class=\"data row1 col2\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col3\" class=\"data row1 col3\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col4\" class=\"data row1 col4\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col5\" class=\"data row1 col5\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row1_col6\" class=\"data row1 col6\" >entailment</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e49258e_8271_11ea_94a4_acbc32902497level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col0\" class=\"data row2 col0\" >masses they church songs church. joyous book to as at choir sings sing from This a the the</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col1\" class=\"data row2 col1\" >game. A at a baseball singing choir</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col2\" class=\"data row2 col2\" >contradiction</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col3\" class=\"data row2 col3\" >contradiction</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col4\" class=\"data row2 col4\" >contradiction</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col5\" class=\"data row2 col5\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row2_col6\" class=\"data row2 col6\" >contradiction</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e49258e_8271_11ea_94a4_acbc32902497level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col0\" class=\"data row3 col0\" >A big and shirt a woman headscarf, a with blue very green grin.</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col1\" class=\"data row3 col1\" >is The young. woman</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col2\" class=\"data row3 col2\" >neutral</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col3\" class=\"data row3 col3\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col4\" class=\"data row3 col4\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col5\" class=\"data row3 col5\" >neutral</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row3_col6\" class=\"data row3 col6\" >neutral</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e49258e_8271_11ea_94a4_acbc32902497level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col0\" class=\"data row4 col0\" >green big with A woman headscarf, a grin. shirt a blue very and</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col1\" class=\"data row4 col1\" >woman is happy. very The</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col2\" class=\"data row4 col2\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col3\" class=\"data row4 col3\" >neutral</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col4\" class=\"data row4 col4\" >entailment</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col5\" class=\"data row4 col5\" >neutral</td>\n",
       "                        <td id=\"T_1e49258e_8271_11ea_94a4_acbc32902497row4_col6\" class=\"data row4 col6\" >neutral</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a41893d50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_ex = df2.head(5)\n",
    "df2_ex.style.set_properties(subset=['Premise', 'Hypothesis'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples scrambled in test set: 9824\n",
      "\n",
      "Overall classification accuracy using AVERAGE encoder: 72.6%\n",
      "Accuracy using AVERAGE encoder on predicting relation: contradiction: 70.37%\n",
      "Accuracy using AVERAGE encoder on predicting relation: entailment: 78.95%\n",
      "Accuracy using AVERAGE encoder on predicting relation: neutral: 68.19%\n",
      "\n",
      "\n",
      "Overall classification accuracy using UNI_LSTM encoder: 72.78%\n",
      "Accuracy using UNI_LSTM encoder on predicting relation: contradiction: 74.08%\n",
      "Accuracy using UNI_LSTM encoder on predicting relation: entailment: 71.29%\n",
      "Accuracy using UNI_LSTM encoder on predicting relation: neutral: 73.04%\n",
      "\n",
      "\n",
      "Overall classification accuracy using BI_LSTM encoder: 71.06%\n",
      "Accuracy using BI_LSTM encoder on predicting relation: contradiction: 71.42%\n",
      "Accuracy using BI_LSTM encoder on predicting relation: entailment: 74.17%\n",
      "Accuracy using BI_LSTM encoder on predicting relation: neutral: 67.44%\n",
      "\n",
      "\n",
      "Overall classification accuracy using BI_LSTM_MP encoder: 78.65%\n",
      "Accuracy using BI_LSTM_MP encoder on predicting relation: contradiction: 81.71%\n",
      "Accuracy using BI_LSTM_MP encoder on predicting relation: entailment: 77.05%\n",
      "Accuracy using BI_LSTM_MP encoder on predicting relation: neutral: 77.26%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total number of examples scrambled in test set: ' + str(len(df2)) +'\\n')\n",
    "\n",
    "# Compute classification accuracy for each relation, for each model\n",
    "models = ['AVERAGE', 'UNI_LSTM', 'BI_LSTM', 'BI_LSTM_MP']\n",
    "categories = [contradictions, entailments, neutrals]\n",
    "for model in models:\n",
    "    print('Overall classification accuracy using '+model+' encoder: '+str(calc_accuracy(model,df2)) +'%')\n",
    "    for category in categories:\n",
    "        print('Accuracy using ' + \n",
    "              str(model) + ' encoder on predicting relation: ' + \n",
    "              category.gold_label.iloc[0] + \n",
    "              ': ' + str(calc_accuracy(model, category))\n",
    "              + '%')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Perturbation Study: Results\n",
    "\n",
    "As expected, shuffling the words of sentences does not affect the performance of the average embedding encoder; it performs exactly the same when used to predict relations for the unaltered sentences. Furthermore, we can see a stark drop in the performance of both the uni-LSTM and bi-LSTMs; their accuracies become comparable to that of the average embedding encoder. \n",
    "\n",
    "Arguably, this suggests that as expected, the uni-LSTM and bi-LSTM are largely able to outperform average embedding models on unaltered sentences by virtue of them being able to account for word order. However, one might verify this theory by also training these encoders on shuffled sentences, and then seeing whether the equalities of performance still hold. It also makes sense that they \n",
    "\n",
    "Surprisingly however, is the performance of the BI_LSTM_MP encoder; despite not being able to rely on word order given the shuffled sentences, it still outperforms the other models at (almost) all levels (though interestingly, the average word embedding encoder performs the highest for predicting contradiction here). This seems to suggest that taking the max-pool over word-level features allows the model to construct a slightly more meaningful sentence representation, when compared to just using the last hidden state(s) - even in the absence of word order. \n",
    "\n",
    "We can also observe that compared to the previous results (for unaltered sentences), there no longer seem to be performance patterns which hold for all models (i.e. models consistently doing better on 'entailment', and worse on 'neutral', as was discussed previously)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
